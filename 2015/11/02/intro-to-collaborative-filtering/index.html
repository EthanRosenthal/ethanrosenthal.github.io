<!DOCTYPE html>
<html lang="en-US">
  <head>
  

  <title>
    
    
        Intro to Recommender Systems: Collaborative Filtering | ethan rosenthal
    
  </title>

  <meta name="title" content="Intro to Recommender Systems: Collaborative Filtering | ethan rosenthal">

  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="referrer" content="no-referrer-when-downgrade">
  <meta name="generator" content="">
  <base href="http://ethanrosenthal.com">

  
    <meta name="description" content="I&rsquo;ve written before about how much I enjoyed Andrew Ng&rsquo;s Coursera Machine Learning course. However, I also mentioned that I thought the course to be lacking a bit in the area of recommender systems. After learning basic models for regression and classification, recommmender systems likely complete the triumvirate of machine learning pillars for data science.
Working at an ecommmerce company, I think a lot about recommender systems and would like to provide an introduction to basic recommendation models.">
  

  
    <meta name="author" content="Ethan Rosenthal">
  

  
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:site" content="@eprosenthal">
    <meta name="twitter:creator" content="@eprosenthal">
  

  <meta property="og:title" content="Intro to Recommender Systems: Collaborative Filtering | ethan rosenthal">
  <meta property="og:type" content="website">
  <meta property="og:url" content="http://ethanrosenthal.com">

  
    <meta property="og:image" content="http://ethanrosenthal.com/images/profile_pic2.jpg">
  

  
    <meta name="og:description" content="I&rsquo;ve written before about how much I enjoyed Andrew Ng&rsquo;s Coursera Machine Learning course. However, I also mentioned that I thought the course to be lacking a bit in the area of recommender systems. After learning basic models for regression and classification, recommmender systems likely complete the triumvirate of machine learning pillars for data science.
Working at an ecommmerce company, I think a lot about recommender systems and would like to provide an introduction to basic recommendation models.">
  

  
    <link rel="icon" type="image/png" sizes="16x16" href="favicon.ico">
    <meta name="theme-color" content="#FFF">
  

  <link rel="canonical" href="http://ethanrosenthal.com/2015/11/02/intro-to-collaborative-filtering/">

  

  <link rel="stylesheet" href="http://ethanrosenthal.com/styles/main.css" type="text/css">

  

  
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
  

  
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-rc.1/dist/katex.min.css" integrity="sha384-D+9gmBxUQogRLqvARvNLmA9hS2x//eK1FhVb9PiU86gmcrBrJAQT8okdJ4LMp2uv" crossorigin="anonymous">
  

  
    <link href="http://ethanrosenthal.comcss/dataframe.css" rel="stylesheet" type="text/css">
  

  
</head>

  <body>
    



<nav class="row middle-xs center-xs">
  <div class="col-xs-6 col-sm-1 logo">
    <a href="http://ethanrosenthal.com#"><img src="http://ethanrosenthal.com/images/profile_pic2.jpg" alt="ethan rosenthal"></a>
  </div>
    
      <div class="col-xs-3 col-sm-2">
        <h3><a href="http://ethanrosenthal.com/#blog">Blog</a></h3>
      </div>
    
      <div class="col-xs-3 col-sm-2">
        <h3><a href="https://physics.ethanrosenthal.com">Physics</a></h3>
      </div>
    
      <div class="col-xs-3 col-sm-2">
        <h3><a href="http://ethanrosenthal.com/#about">About</a></h3>
      </div>
    
  <div class="col-xs-6 col-sm-1 nav-toggle">
      <a href="" class="nav-icon" onclick="return false"><img src="http://ethanrosenthal.com/images/icon-menu.png" alt="Open Menu"><img src="http://ethanrosenthal.com/images/icon-x.png" alt="Close Menu" style="display: none;"></a>
  </div>
</nav>

<section class="nav-full row middle-xs center-xs">
  <div class="col-xs-12">
    <div class="row middle-xs center-xs">
      
        <div class="col-xs-12"><h1><a href="http://ethanrosenthal.com/#blog">Blog</a></h1></div>
      
        <div class="col-xs-12"><h1><a href="https://physics.ethanrosenthal.com">Physics</a></h1></div>
      
        <div class="col-xs-12"><h1><a href="http://ethanrosenthal.com/#about">About</a></h1></div>
      
    </div>
  </div>
</section>

    <main>

      <section class="container">
          <section class="content">
              <h1> Intro to Recommender Systems: Collaborative Filtering </h1>

              <div class="sub-header">
                  November 2, 2015 Â· 16 minute read
              </div>

              <article class="entry-content">
                  

<div class="jupyter-cell markdown">


<!-- PELICAN_BEGIN_SUMMARY -->

<p>I&rsquo;ve written before about how much I enjoyed Andrew Ng&rsquo;s Coursera Machine Learning course. However, I also mentioned that I thought the course to be lacking a bit in the area of recommender systems. After learning basic models for regression and classification, recommmender systems likely complete the triumvirate of machine learning pillars for data science.</p>

<!-- PELICAN_END_SUMMARY -->

<p>Working at an ecommmerce company, I think a lot about recommender systems and would like to provide an introduction to basic recommendation models. The goal of a recommendation model is to present a ranked list of objects given an input object. Typically, this ranking is based on the similarity between the input object and the listed objects. To be less vague, one often wants to either present similar products to a given product or present products that are personally recommended for a given user.</p>

<p>The astounding thing is that if one has enough user-to-product data (ratings, purchases, etc&hellip;), then no other information is necessary to make decent recommendations. This is quite different than regression and classification problems where one must explore various features in order to boost a model&rsquo;s predictive powers.</p>

<p>For this introduction, I&rsquo;ll use the MovieLens dataset - a classic dataset for training recommendation models. It can be obtained from the GroupLens website. There are various datasets, but the one that I will use below consists of 100,000 movie ratings by users (on a 1-5 scale). The main data file consists of a tab-separated list with user-id (starting at 1), item-id (starting at 1), rating, and timestamp as the four fields. We can use bash commands in the Jupyter notebook to download the file and then read it in with pandas.</p>

<p></div>
<div class="jupyter-cell code">
</p>

<div class="jupyter-input">


<pre><code class="language-python">import numpy as np
import pandas as pd
</code></pre>

</div>


<p></div>
<div class="jupyter-cell code">
</p>

<div class="jupyter-input">


<pre><code class="language-python"># !curl -O http://files.grouplens.org/datasets/movielens/ml-100k.zip
# !unzip ml-100k.zip
</code></pre>

</div>


<p></div>
<div class="jupyter-cell code">
</p>

<div class="jupyter-input">


<pre><code class="language-python">cd ml-100k/
</code></pre>

</div>


<p></div>
<div class="jupyter-cell code">
</p>

<div class="jupyter-input">


<pre><code class="language-python">ls
</code></pre>

</div>


<pre><code>[0m[01;32mallbut.pl[0m*        u1.base  u3.base  u5.base  ub.base  u.info
grid_search.cpkl  u1.test  u3.test  u5.test  ub.test  u.item
[01;32mmku.sh[0m*           u2.base  u4.base  ua.base  u.data   u.occupation
README            u2.test  u4.test  ua.test  u.genre  u.user
</code></pre>

<p></div>
<div class="jupyter-cell code">
</p>

<div class="jupyter-input">


<pre><code class="language-python">!head u.data
!echo # line break
!wc -l u.data
</code></pre>

</div>


<pre><code>196 242 3   881250949
186 302 3   891717742
22  377 1   878887116
244 51  2   880606923
166 346 1   886397596
298 474 4   884182806
115 265 2   881171488
253 465 5   891628467
305 451 3   886324817
6   86  3   883603013

100000 u.data
</code></pre>

<p></div>
<div class="jupyter-cell code">
</p>

<div class="jupyter-input">


<pre><code class="language-python">names = ['user_id', 'item_id', 'rating', 'timestamp']
df = pd.read_csv('u.data', sep='\t', names=names)
df.head()
</code></pre>

</div>


<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>user_id</th>
      <th>item_id</th>
      <th>rating</th>
      <th>timestamp</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>196</td>
      <td>242</td>
      <td>3</td>
      <td>881250949</td>
    </tr>
    <tr>
      <th>1</th>
      <td>186</td>
      <td>302</td>
      <td>3</td>
      <td>891717742</td>
    </tr>
    <tr>
      <th>2</th>
      <td>22</td>
      <td>377</td>
      <td>1</td>
      <td>878887116</td>
    </tr>
    <tr>
      <th>3</th>
      <td>244</td>
      <td>51</td>
      <td>2</td>
      <td>880606923</td>
    </tr>
    <tr>
      <th>4</th>
      <td>166</td>
      <td>346</td>
      <td>1</td>
      <td>886397596</td>
    </tr>
  </tbody>
</table>
</div>

<p></div>
<div class="jupyter-cell code">
</p>

<div class="jupyter-input">


<pre><code class="language-python">n_users = df.user_id.unique().shape[0]
n_items = df.item_id.unique().shape[0]
print str(n_users) + ' users'
print str(n_items) + ' items'
</code></pre>

</div>


<pre><code>943 users
1682 items
</code></pre>

<p></div>
<div class="jupyter-cell markdown">
</p>

<p>Most recommendation models consist of building a user-by-item matrix with some sort of &ldquo;interaction&rdquo; number in each cell. If one includes the numerical ratings that users give items, then this is called an <em>explicit feedback</em> model. Alternatively, one may include <em>implicit feedback</em> which are actions by a user that signify a positive or negative preference for a given item (such as viewing the item online). These two scenarios often must be treated differently.</p>

<p>In the case of the MovieLens dataset, we have ratings, so we will focus on explicit feedback models. First, we must construct our user-item matrix. We can easily map user/item ID&rsquo;s to user/item indices by removing the &ldquo;Python starts at 0&rdquo; offset between them.</p>

<p></div>
<div class="jupyter-cell code">
</p>

<div class="jupyter-input">


<pre><code class="language-python">ratings = np.zeros((n_users, n_items))
for row in df.itertuples():
    ratings[row[1]-1, row[2]-1] = row[3]
ratings
</code></pre>

</div>


<pre><code>array([[ 5.,  3.,  4., ...,  0.,  0.,  0.],
       [ 4.,  0.,  0., ...,  0.,  0.,  0.],
       [ 0.,  0.,  0., ...,  0.,  0.,  0.],
       ..., 
       [ 5.,  0.,  0., ...,  0.,  0.,  0.],
       [ 0.,  0.,  0., ...,  0.,  0.,  0.],
       [ 0.,  5.,  0., ...,  0.,  0.,  0.]])
</code></pre>

<p></div>
<div class="jupyter-cell code">
</p>

<div class="jupyter-input">


<pre><code class="language-python">sparsity = float(len(ratings.nonzero()[0]))
sparsity /= (ratings.shape[0] * ratings.shape[1])
sparsity *= 100
print 'Sparsity: {:4.2f}%'.format(sparsity)
</code></pre>

</div>


<pre><code>Sparsity: 6.30%
</code></pre>

<p></div>
<div class="jupyter-cell markdown">
</p>

<p>In this dataset, every user has rated at least 20 movies which results in a reasonable sparsity of 6.3%. This means that 6.3% of the user-item ratings have a value. Note that, although we filled in missing ratings as 0, we should not assume these values to truly be zero. More appropriately, they are just empty entries. We will split our data into training and test sets by removing 10 ratings per user from the training set and placing them in the test set.</p>

<p></div>
<div class="jupyter-cell code">
</p>

<div class="jupyter-input">


<pre><code class="language-python">def train_test_split(ratings):
    test = np.zeros(ratings.shape)
    train = ratings.copy()
    for user in xrange(ratings.shape[0]):
        test_ratings = np.random.choice(ratings[user, :].nonzero()[0], 
                                        size=10, 
                                        replace=False)
        train[user, test_ratings] = 0.
        test[user, test_ratings] = ratings[user, test_ratings]
        
    # Test and training are truly disjoint
    assert(np.all((train * test) == 0)) 
    return train, test
</code></pre>

</div>


<p></div>
<div class="jupyter-cell code">
</p>

<div class="jupyter-input">


<pre><code class="language-python">train, test = train_test_split(ratings)
</code></pre>

</div>


<p></div>
<div class="jupyter-cell markdown">
</p>

<h2 id="collaborative-filtering">Collaborative filtering</h2>

<p>We will focus on collaborative filtering models today which can be generally split into two classes: user- and item-based collaborative filtering. In either scenario, one builds a similarity matrix. For user-based collaborative filtering, the user-similarity matrix will consist of some distance metric that measures the similarity between any two pairs of users. Likewise, the item-similarity matrix will measure the similarity between any two pairs of items.</p>

<p>A common distance metric is cosine similarity. The metric can be thought of geometrically if one treats a given user&rsquo;s (item&rsquo;s) row (column) of the ratings matrix as a vector. For user-based collaborative filtering, two users&rsquo; similarity is measured as the cosine of the angle between the two users&rsquo; vectors. For users ${u}$ and ${u^{\prime}}$, the cosine similarity is</p>

<p>$$
sim(u, u^{\prime}) =
cos(\theta{}) =
\frac{\textbf{r}_{u} \dot{} \textbf{r}_{u^{\prime}}}{| \textbf{r}_{u} | | \textbf{r}_{u^{\prime}} |} =
\sum_{i} \frac{r_{ui}r_{u^{\prime}i}}{\sqrt{\sum\limits_{i} r_{ui}^2} \sqrt{\sum\limits_{i} r_{u^{\prime}i}^2} }
$$</p>

<p>This can be written as a for-loop with code, but the Python code will run quite slow; instead, one should try to express any equation in terms of NumPy functions. I&rsquo;ve included a slow and fast version of the cosine similarity function below. The slow function took so long that I eventually canceled it because I got tired of waiting. The fast function, on the other hand, takes around 200 ms.</p>

<p>The cosine similarity will range from 0 to 1 in our case (because there are no negative ratings). Notice that it is symmetric and has ones along the diagonal.</p>

<p></div>
<div class="jupyter-cell code">
</p>

<div class="jupyter-input">


<pre><code class="language-python">def slow_similarity(ratings, kind='user'):
    if kind == 'user':
        axmax = 0
        axmin = 1
    elif kind == 'item':
        axmax = 1
        axmin = 0
    sim = np.zeros((ratings.shape[axmax], ratings.shape[axmax]))
    for u in xrange(ratings.shape[axmax]):
        for uprime in xrange(ratings.shape[axmax]):
            rui_sqrd = 0.
            ruprimei_sqrd = 0.
            for i in xrange(ratings.shape[axmin]):
                sim[u, uprime] = ratings[u, i] * ratings[uprime, i]
                rui_sqrd += ratings[u, i] ** 2
                ruprimei_sqrd += ratings[uprime, i] ** 2
            sim[u, uprime] /= rui_sqrd * ruprimei_sqrd
    return sim

def fast_similarity(ratings, kind='user', epsilon=1e-9):
    # epsilon -&gt; small number for handling dived-by-zero errors
    if kind == 'user':
        sim = ratings.dot(ratings.T) + epsilon
    elif kind == 'item':
        sim = ratings.T.dot(ratings) + epsilon
    norms = np.array([np.sqrt(np.diagonal(sim))])
    return (sim / norms / norms.T)
</code></pre>

</div>


<p></div>
<div class="jupyter-cell code">
</p>

<div class="jupyter-input">


<pre><code class="language-python">#%timeit slow_user_similarity(train)
</code></pre>

</div>


<p></div>
<div class="jupyter-cell code">
</p>

<div class="jupyter-input">


<pre><code class="language-python">%timeit fast_similarity(train, kind='user')
</code></pre>

</div>


<pre><code>1 loop, best of 3: 206 ms per loop
</code></pre>

<p></div>
<div class="jupyter-cell code">
</p>

<div class="jupyter-input">


<pre><code class="language-python">user_similarity = fast_similarity(train, kind='user')
item_similarity = fast_similarity(train, kind='item')
print item_similarity[:4, :4]
</code></pre>

</div>


<pre><code>[[ 1.          0.4142469   0.33022352  0.44198521]
 [ 0.4142469   1.          0.26600176  0.48216178]
 [ 0.33022352  0.26600176  1.          0.3011288 ]
 [ 0.44198521  0.48216178  0.3011288   1.        ]]
</code></pre>

<p></div>
<div class="jupyter-cell markdown">
</p>

<p>With our similarity matrix in hand, we can now predict the ratings that were not included with the data. Using these predictions, we can then compare them with the test data to attempt to validate the quality of our recommender model.</p>

<p>For user-based collaborative filtering, we predict that a user&rsquo;s $u$&rsquo;s rating for item $i$ is given by the weighted sum of all other users&rsquo; ratings for item $i$ where the weighting is the cosine similarity between the each user and the input user $u$.</p>

<p>$$\hat{r}_{ui} = \sum\limits_{u^{\prime}}sim(u, u^{\prime}) r_{u^{\prime}i}$$</p>

<p>We must also normalize by the number of ${r_{u^{\prime}i}}$ ratings:</p>

<p>$$\hat{r}_{ui} = \frac{\sum\limits_{u^{\prime}} sim(u, u^{\prime}) r_{u^{\prime}i}}{\sum\limits_{u^{\prime}}|sim(u, u^{\prime})|}$$</p>

<p>As with before, our computational speed will benefit greatly by favoring NumPy functions over for loops. With our slow function below, even though I use NumPy methods, the presence of the for-loop still slows the algorithm</p>

<p></div>
<div class="jupyter-cell code">
</p>

<div class="jupyter-input">


<pre><code class="language-python">def predict_slow_simple(ratings, similarity, kind='user'):
    pred = np.zeros(ratings.shape)
    if kind == 'user':
        for i in xrange(ratings.shape[0]):
            for j in xrange(ratings.shape[1]):
                pred[i, j] = similarity[i, :].dot(ratings[:, j])\
                             /np.sum(np.abs(similarity[i, :]))
        return pred
    elif kind == 'item':
        for i in xrange(ratings.shape[0]):
            for j in xrange(ratings.shape[1]):
                pred[i, j] = similarity[j, :].dot(ratings[i, :].T)\
                             /np.sum(np.abs(similarity[j, :]))

        return pred

def predict_fast_simple(ratings, similarity, kind='user'):
    if kind == 'user':
        return similarity.dot(ratings) / np.array([np.abs(similarity).sum(axis=1)]).T
    elif kind == 'item':
        return ratings.dot(similarity) / np.array([np.abs(similarity).sum(axis=1)])
</code></pre>

</div>


<p></div>
<div class="jupyter-cell code">
</p>

<div class="jupyter-input">


<pre><code class="language-python">%timeit predict_slow_simple(train, user_similarity, kind='user')
</code></pre>

</div>


<pre><code>1 loop, best of 3: 33.7 s per loop
</code></pre>

<p></div>
<div class="jupyter-cell code">
</p>

<div class="jupyter-input">


<pre><code class="language-python">%timeit predict_fast_simple(train, user_similarity, kind='user')
</code></pre>

</div>


<pre><code>1 loop, best of 3: 188 ms per loop
</code></pre>

<p></div>
<div class="jupyter-cell markdown">
</p>

<p>We&rsquo;ll use the scikit-learn&rsquo;s mean squared error function as our validation metric. Comparing user- and item-based collaborative filtering, it looks like user-based collaborative filtering gives us a better result.</p>

<p></div>
<div class="jupyter-cell code">
</p>

<div class="jupyter-input">


<pre><code class="language-python">from sklearn.metrics import mean_squared_error

def get_mse(pred, actual):
    # Ignore nonzero terms.
    pred = pred[actual.nonzero()].flatten()
    actual = actual[actual.nonzero()].flatten()
    return mean_squared_error(pred, actual)
</code></pre>

</div>


<p></div>
<div class="jupyter-cell code">
</p>

<div class="jupyter-input">


<pre><code class="language-python">item_prediction = predict_fast_simple(train, item_similarity, kind='item')
user_prediction = predict_fast_simple(train, user_similarity, kind='user')

print 'User-based CF MSE: ' + str(get_mse(user_prediction, test))
print 'Item-based CF MSE: ' + str(get_mse(item_prediction, test))
</code></pre>

</div>


<pre><code>User-based CF MSE: 8.39140463389
Item-based CF MSE: 11.5469590109
</code></pre>

<p></div>
<div class="jupyter-cell markdown">
</p>

<h3 id="top-k-collaborative-filtering">Top-$k$ Collaborative Filtering</h3>

<p>We can attempt to improve our prediction MSE by only considering the top $k$ users who are most similar to the input user (or, similarly, the top $k$ items). That is, when we calculate the sums over $u^{\prime}$</p>

<p>$$\hat{r}_{ui} = \frac{\sum\limits_{u^{\prime}} sim(u, u^{\prime}) r_{u^{\prime}i}}{\sum\limits_{u^{\prime}}|sim(u, u^{\prime})|}$$</p>

<p>we only sum over the top $k$ <em>most similar</em> users. A slow implementation of this algorithm is shown below. While I am sure that there is a way to use numpy sorting to get rid of the double for-loops, I got pretty frustrated desciphering the 2D argsort output and just settled for the slow loop.</p>

<p>As is shown below, employing this method actually halves our error!</p>

<p></div>
<div class="jupyter-cell code">
</p>

<div class="jupyter-input">


<pre><code class="language-python">def predict_topk(ratings, similarity, kind='user', k=40):
    pred = np.zeros(ratings.shape)
    if kind == 'user':
        for i in xrange(ratings.shape[0]):
            top_k_users = [np.argsort(similarity[:,i])[:-k-1:-1]]
            for j in xrange(ratings.shape[1]):
                pred[i, j] = similarity[i, :][top_k_users].dot(ratings[:, j][top_k_users]) 
                pred[i, j] /= np.sum(np.abs(similarity[i, :][top_k_users]))
    if kind == 'item':
        for j in xrange(ratings.shape[1]):
            top_k_items = [np.argsort(similarity[:,j])[:-k-1:-1]]
            for i in xrange(ratings.shape[0]):
                pred[i, j] = similarity[j, :][top_k_items].dot(ratings[i, :][top_k_items].T) 
                pred[i, j] /= np.sum(np.abs(similarity[j, :][top_k_items]))        
    
    return pred
</code></pre>

</div>


<p></div>
<div class="jupyter-cell code">
</p>

<div class="jupyter-input">


<pre><code class="language-python">pred = predict_topk(train, user_similarity, kind='user', k=40)
print 'Top-k User-based CF MSE: ' + str(get_mse(pred, test))

pred = predict_topk(train, item_similarity, kind='item', k=40)
print 'Top-k Item-based CF MSE: ' + str(get_mse(pred, test))
</code></pre>

</div>


<pre><code>Top-k User-based CF MSE: 6.47059807493
Top-k Item-based CF MSE: 7.75559095568
</code></pre>

<p></div>
<div class="jupyter-cell markdown">
</p>

<p>We can try tuning the parameter of $k$ to find the optimal value for minimizing our testing MSE. Here, it often helps to visualize the results to get a feeling for what&rsquo;s going on.</p>

<p></div>
<div class="jupyter-cell code">
</p>

<div class="jupyter-input">


<pre><code class="language-python">k_array = [5, 15, 30, 50, 100, 200]
user_train_mse = []
user_test_mse = []
item_test_mse = []
item_train_mse = []

def get_mse(pred, actual):
    pred = pred[actual.nonzero()].flatten()
    actual = actual[actual.nonzero()].flatten()
    return mean_squared_error(pred, actual)

for k in k_array:
    user_pred = predict_topk(train, user_similarity, kind='user', k=k)
    item_pred = predict_topk(train, item_similarity, kind='item', k=k)
    
    user_train_mse += [get_mse(user_pred, train)]
    user_test_mse += [get_mse(user_pred, test)]
    
    item_train_mse += [get_mse(item_pred, train)]
    item_test_mse += [get_mse(item_pred, test)]  
</code></pre>

</div>


<p></div>
<div class="jupyter-cell code">
</p>

<div class="jupyter-input">


<pre><code class="language-python">%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
sns.set()

pal = sns.color_palette(&quot;Set2&quot;, 2)

plt.figure(figsize=(8, 8))
plt.plot(k_array, user_train_mse, c=pal[0], label='User-based train', alpha=0.5, linewidth=5)
plt.plot(k_array, user_test_mse, c=pal[0], label='User-based test', linewidth=5)
plt.plot(k_array, item_train_mse, c=pal[1], label='Item-based train', alpha=0.5, linewidth=5)
plt.plot(k_array, item_test_mse, c=pal[1], label='Item-based test', linewidth=5)
plt.legend(loc='best', fontsize=20)
plt.xticks(fontsize=16);
plt.yticks(fontsize=16);
plt.xlabel('k', fontsize=30);
plt.ylabel('MSE', fontsize=30);
</code></pre>

</div>





<figure>
    
        <img src="http://ethanrosenthal.com/2015/11/02/intro-to-collaborative-filtering/./index_31_0.png"/> </figure>


<p></div>
<div class="jupyter-cell markdown">
</p>

<p>It looks like a <em>k</em> of 50 and 15 produces a nice minimum in the test error for user- and item-based collaborative filtering, respectively.</p>

<h3 id="bias-subtracted-collaborative-filtering">Bias-subtracted Collaborative Filtering</h3>

<p>For our last method of improving recommendations, we will try removing biases associated with either the user of the item. The idea here is that certain users may tend to always give high or low ratings to all movies. One could imagine that the <em>relative difference</em> in the ratings that these users give is more important than the <em>absolute</em> rating values.</p>

<p>Let us try subtracting each user&rsquo;s average rating when summing over similar user&rsquo;s ratings and then add that average back in at the end. Mathematically, this looks like</p>

<p>$$\hat{r}_{ui} = \bar{r_{u}} + \frac{\sum\limits_{u^{\prime}} sim(u, u^{\prime}) (r_{u^{\prime}i} - \bar{r_{u^{\prime}}})}{\sum\limits_{u^{\prime}}|sim(u, u^{\prime})|}$$</p>

<p>where $\bar{r_{u}}$ is user $u$&rsquo;s average rating.</p>

<p></div>
<div class="jupyter-cell code">
</p>

<div class="jupyter-input">


<pre><code class="language-python">def predict_nobias(ratings, similarity, kind='user'):
    if kind == 'user':
        user_bias = ratings.mean(axis=1)
        ratings = (ratings - user_bias[:, np.newaxis]).copy()
        pred = similarity.dot(ratings) / np.array([np.abs(similarity).sum(axis=1)]).T
        pred += user_bias[:, np.newaxis]
    elif kind == 'item':
        item_bias = ratings.mean(axis=0)
        ratings = (ratings - item_bias[np.newaxis, :]).copy()
        pred = ratings.dot(similarity) / np.array([np.abs(similarity).sum(axis=1)])
        pred += item_bias[np.newaxis, :]
        
    return pred
</code></pre>

</div>


<p></div>
<div class="jupyter-cell code">
</p>

<div class="jupyter-input">


<pre><code class="language-python">user_pred = predict_nobias(train, user_similarity, kind='user')
print 'Bias-subtracted User-based CF MSE: ' + str(get_mse(user_pred, test))

item_pred = predict_nobias(train, item_similarity, kind='item')
print 'Bias-subtracted Item-based CF MSE: ' + str(get_mse(item_pred, test))
</code></pre>

</div>


<pre><code>Bias-subtracted User-based CF MSE: 8.67647634245
Bias-subtracted Item-based CF MSE: 9.71148412222
</code></pre>

<p></div>
<div class="jupyter-cell markdown">
</p>

<h3 id="all-together-now">All together now</h3>

<p>Finally, we can try combining both the Top-<em>k</em> and the Bias-subtracted algorithms. Strangely enough, this actually seems to perform worse than the original Top-<em>k</em> algorithm. Go figure.</p>

<p></div>
<div class="jupyter-cell code">
</p>

<div class="jupyter-input">


<pre><code class="language-python">def predict_topk_nobias(ratings, similarity, kind='user', k=40):
    pred = np.zeros(ratings.shape)
    if kind == 'user':
        user_bias = ratings.mean(axis=1)
        ratings = (ratings - user_bias[:, np.newaxis]).copy()
        for i in xrange(ratings.shape[0]):
            top_k_users = [np.argsort(similarity[:,i])[:-k-1:-1]]
            for j in xrange(ratings.shape[1]):
                pred[i, j] = similarity[i, :][top_k_users].dot(ratings[:, j][top_k_users]) 
                pred[i, j] /= np.sum(np.abs(similarity[i, :][top_k_users]))
        pred += user_bias[:, np.newaxis]
    if kind == 'item':
        item_bias = ratings.mean(axis=0)
        ratings = (ratings - item_bias[np.newaxis, :]).copy()
        for j in xrange(ratings.shape[1]):
            top_k_items = [np.argsort(similarity[:,j])[:-k-1:-1]]
            for i in xrange(ratings.shape[0]):
                pred[i, j] = similarity[j, :][top_k_items].dot(ratings[i, :][top_k_items].T) 
                pred[i, j] /= np.sum(np.abs(similarity[j, :][top_k_items])) 
        pred += item_bias[np.newaxis, :]
        
    return pred
</code></pre>

</div>


<p></div>
<div class="jupyter-cell code">
</p>

<div class="jupyter-input">


<pre><code class="language-python">k_array = [5, 15, 30, 50, 100, 200]
user_train_mse = []
user_test_mse = []
item_test_mse = []
item_train_mse = []

for k in k_array:
    user_pred = predict_topk_nobias(train, user_similarity, kind='user', k=k)
    item_pred = predict_topk_nobias(train, item_similarity, kind='item', k=k)
    
    user_train_mse += [get_mse(user_pred, train)]
    user_test_mse += [get_mse(user_pred, test)]
    
    item_train_mse += [get_mse(item_pred, train)]
    item_test_mse += [get_mse(item_pred, test)]  
</code></pre>

</div>


<p></div>
<div class="jupyter-cell code">
</p>

<div class="jupyter-input">


<pre><code class="language-python">pal = sns.color_palette(&quot;Set2&quot;, 2)

plt.figure(figsize=(8, 8))
plt.plot(k_array, user_train_mse, c=pal[0], label='User-based train', alpha=0.5, linewidth=5)
plt.plot(k_array, user_test_mse, c=pal[0], label='User-based test', linewidth=5)
plt.plot(k_array, item_train_mse, c=pal[1], label='Item-based train', alpha=0.5, linewidth=5)
plt.plot(k_array, item_test_mse, c=pal[1], label='Item-based test', linewidth=5)
plt.legend(loc='best', fontsize=20)
plt.xticks(fontsize=16);
plt.yticks(fontsize=16);
plt.xlabel('k', fontsize=30);
plt.ylabel('MSE', fontsize=30);
</code></pre>

</div>





<figure>
    
        <img src="http://ethanrosenthal.com/2015/11/02/intro-to-collaborative-filtering/./index_38_0.png"/> </figure>


<p></div>
<div class="jupyter-cell markdown">
</p>

<h2 id="validation">Validation</h2>

<p>Having expanded upon the basic collaborative filtering algorithm, I have shown how we can reduce our mean squared error with increasing model complexity. However, how do we really know if we are making good recommendations? One thing that I glossed over was our choice of similarity metric. How do we know that cosine similarity was a good metric to use? Because we are dealing with a domain where many of us have intuition (movies), we can look at our item similarity matrix and see if similar items &ldquo;make sense&rdquo;.</p>

<p>And just for fun, let us really <em>look</em> at the items. The MovieLens dataset contains a file with information about each movie. It turns out that there is a website called <a href="https://www.themoviedb.org/">themoviedb.org</a> which has a free API. If we have the IMDB &ldquo;movie id&rdquo; for a movie, then we can use this API to return the posters of movies. Looking at the movie data file below, it seems that we at least have the IMDB url for each movie.</p>

<p></div>
<div class="jupyter-cell code">
</p>

<div class="jupyter-input">


<pre><code class="language-python">!head -5 u.item
</code></pre>

</div>


<pre><code>1|Toy Story (1995)|01-Jan-1995||http://us.imdb.com/M/title-exact?Toy%20Story%20(1995)|0|0|0|1|1|1|0|0|0|0|0|0|0|0|0|0|0|0|0
2|GoldenEye (1995)|01-Jan-1995||http://us.imdb.com/M/title-exact?GoldenEye%20(1995)|0|1|1|0|0|0|0|0|0|0|0|0|0|0|0|0|1|0|0
3|Four Rooms (1995)|01-Jan-1995||http://us.imdb.com/M/title-exact?Four%20Rooms%20(1995)|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|1|0|0
4|Get Shorty (1995)|01-Jan-1995||http://us.imdb.com/M/title-exact?Get%20Shorty%20(1995)|0|1|0|0|0|1|0|0|1|0|0|0|0|0|0|0|0|0|0
5|Copycat (1995)|01-Jan-1995||http://us.imdb.com/M/title-exact?Copycat%20(1995)|0|0|0|0|0|0|1|0|1|0|0|0|0|0|0|0|1|0|0
</code></pre>

<p></div>
<div class="jupyter-cell markdown">
</p>

<p>If you follow one of the links in this dataset, then your url will get redirected. The resulting url contains the IMDB movie ID as the last information in the url starting with &ldquo;tt&rdquo;. For example, the redirected url for Toy Story is <a href="http://www.imdb.com/title/tt0114709/">http://www.imdb.com/title/tt0114709/</a>, and the IMDB movie ID is <code>tt0114709</code>.</p>

<p>Using the Python requests library, we can automatically extract this movie ID. The Toy Story example is shown below.</p>

<p></div>
<div class="jupyter-cell code">
</p>

<div class="jupyter-input">


<pre><code class="language-python">import requests
import json

response = requests.get('http://us.imdb.com/M/title-exact?Toy%20Story%20(1995)')
print response.url.split('/')[-2]
</code></pre>

</div>


<pre><code>tt0114709
</code></pre>

<p></div>
<div class="jupyter-cell markdown">
</p>

<p>I requested a free API key from themoviedb.org. The key is necessary for querying the API. I&rsquo;ve omitted it below, so be aware that if you will need your own key if you want to reproduce this. We can search for movie posters by movie id and then grab links to the image files. The links are relative paths, so we need the base_url query at the top of the next cell to get the full path. Also, some of the links don&rsquo;t work, so we can instead search for the movie by title and grab the first result.</p>

<p></div>
<div class="jupyter-cell code">
</p>

<div class="jupyter-input">


<pre><code class="language-python"># Get base url filepath structure. w185 corresponds to size of movie poster.
headers = {'Accept': 'application/json'}
payload = {'api_key': 'INSERT API KEY HERE'} 
response = requests.get(&quot;http://api.themoviedb.org/3/configuration&quot;, params=payload, headers=headers)
response = json.loads(response.text)
base_url = response['images']['base_url'] + 'w185'

def get_poster(imdb_url, base_url):
    # Get IMDB movie ID
    response = requests.get(imdb_url)
    movie_id = response.url.split('/')[-2]
    
    # Query themoviedb.org API for movie poster path.
    movie_url = 'http://api.themoviedb.org/3/movie/{:}/images'.format(movie_id)
    headers = {'Accept': 'application/json'}
    payload = {'api_key': 'INSERT API KEY HERE'} 
    response = requests.get(movie_url, params=payload, headers=headers)
    try:
        file_path = json.loads(response.text)['posters'][0]['file_path']
    except:
        # IMDB movie ID is sometimes no good. Need to get correct one.
        movie_title = imdb_url.split('?')[-1].split('(')[0]
        payload['query'] = movie_title
        response = requests.get('http://api.themoviedb.org/3/search/movie', params=payload, headers=headers)
        movie_id = json.loads(response.text)['results'][0]['id']
        payload.pop('query', None)
        movie_url = 'http://api.themoviedb.org/3/movie/{:}/images'.format(movie_id)
        response = requests.get(movie_url, params=payload, headers=headers)
        file_path = json.loads(response.text)['posters'][0]['file_path']
        
    return base_url + file_path
</code></pre>

</div>


<p></div>
<div class="jupyter-cell code">
</p>

<div class="jupyter-input">


<pre><code class="language-python">from IPython.display import Image
from IPython.display import display

toy_story = 'http://us.imdb.com/M/title-exact?Toy%20Story%20(1995)'
Image(url=get_poster(toy_story, base_url))
</code></pre>

</div>


<p><img src="http://image.tmdb.org/t/p/w185/uMZqKhT4YA6mqo2yczoznv7IDmv.jpg"/></p>

<p></div>
<div class="jupyter-cell markdown">
</p>

<p>Ta-da! Now we have a pipeline to go directly from the IMDB url in the data file to displaying the movie poster. With this machinery in hand, let us investigate our movie similarity matrix.</p>

<p>We can build a dictionary to map the movie-indices from our similarity matrix to the urls of the movies. We&rsquo;ll also create a helper function to return the top-<em>k</em> most similar movies given some input movie. With this function, the first movie returned will be the input movie (because of course it is the most similar to itself).</p>

<p></div>
<div class="jupyter-cell code">
</p>

<div class="jupyter-input">


<pre><code class="language-python"># Load in movie data
idx_to_movie = {}
with open('u.item', 'r') as f:
    for line in f.readlines():
        info = line.split('|')
        idx_to_movie[int(info[0])-1] = info[4]
        
def top_k_movies(similarity, mapper, movie_idx, k=6):
    return [mapper[x] for x in np.argsort(similarity[movie_idx,:])[:-k-1:-1]]
</code></pre>

</div>


<p></div>
<div class="jupyter-cell code">
</p>

<div class="jupyter-input">


<pre><code class="language-python">idx = 0 # Toy Story
movies = top_k_movies(item_similarity, idx_to_movie, idx)
posters = tuple(Image(url=get_poster(movie, base_url)) for movie in movies)
</code></pre>

</div>


<p></div>
<div class="jupyter-cell code">
</p>

<div class="jupyter-input">


<pre><code class="language-python">display(*posters)
</code></pre>

</div>


<p><img src="http://image.tmdb.org/t/p/w185/uMZqKhT4YA6mqo2yczoznv7IDmv.jpg"/></p>

<p><img src="http://image.tmdb.org/t/p/w185/tvSlBzAdRE29bZe5yYWrJ2ds137.jpg"/></p>

<p><img src="http://image.tmdb.org/t/p/w185/jx5p0aHlbPXqe3AH9G15NvmWaqQ.jpg"/></p>

<p><img src="http://image.tmdb.org/t/p/w185/cf3cmVa1zrmfQoltnezvsniNnoX.jpg"/></p>

<p><img src="http://image.tmdb.org/t/p/w185/4fSlleLcmU2DJcibJugh2lLk6Fh.jpg"/></p>

<p><img src="http://image.tmdb.org/t/p/w185/pAVaLJQBRM4JlZEGXvezwbxOd15.jpg"/></p>

<p></div>
<div class="jupyter-cell markdown">
</p>

<p>Hmmm, those recommendations do not seem too good! Let&rsquo;s look at a couple more.</p>

<p></div>
<div class="jupyter-cell code">
</p>

<div class="jupyter-input">


<pre><code class="language-python">idx = 1 # GoldenEye
movies = top_k_movies(item_similarity, idx_to_movie, idx)
posters = tuple(Image(url=get_poster(movie, base_url)) for movie in movies)
display(*posters)
</code></pre>

</div>


<p><img src="http://image.tmdb.org/t/p/w185/trtANqAEy9dxRCeIe7YEDVeGkLw.jpg"/></p>

<p><img src="http://image.tmdb.org/t/p/w185/duxM3HJ55Fpc4A3krziKn1FR22K.jpg"/></p>

<p><img src="http://image.tmdb.org/t/p/w185/orGXnBKfT41LxZhitLkXhqUfJJW.jpg"/></p>

<p><img src="http://image.tmdb.org/t/p/w185/39WsfbB5BshvdbPAYRFXdsjC481.jpg"/></p>

<p><img src="http://image.tmdb.org/t/p/w185/mTAHr5h5i64hTLqo0cW2X2083Cx.jpg"/></p>

<p><img src="http://image.tmdb.org/t/p/w185/8XefYka77ypAnPJvaVlfUGBBs4a.jpg"/></p>

<p></div>
<div class="jupyter-cell code">
</p>

<div class="jupyter-input">


<pre><code class="language-python">idx = 20 # Muppet Treasure Island
movies = top_k_movies(item_similarity, idx_to_movie, idx)
posters = tuple(Image(url=get_poster(movie, base_url)) for movie in movies)
display(*posters)
</code></pre>

</div>


<p><img src="http://image.tmdb.org/t/p/w185/5A8gKzOrF9Z7tSUX6xd5dEx4NXf.jpg"/></p>

<p><img src="http://image.tmdb.org/t/p/w185/k3F8N3jeqXOpm1qjY7mL8O6vdx.jpg"/></p>

<p><img src="http://image.tmdb.org/t/p/w185/7f53XAE4nPiGe9XprpGAeWHuKPw.jpg"/></p>

<p><img src="http://image.tmdb.org/t/p/w185/jx5p0aHlbPXqe3AH9G15NvmWaqQ.jpg"/></p>

<p><img src="http://image.tmdb.org/t/p/w185/kBf3g9crrADGMc2AMAMlLBgSm2h.jpg"/></p>

<p><img src="http://image.tmdb.org/t/p/w185/39WsfbB5BshvdbPAYRFXdsjC481.jpg"/></p>

<p></div>
<div class="jupyter-cell code">
</p>

<div class="jupyter-input">


<pre><code class="language-python">idx = 40 # Billy Madison
movies = top_k_movies(item_similarity, idx_to_movie, idx)
posters = tuple(Image(url=get_poster(movie, base_url)) for movie in movies)
display(*posters)
</code></pre>

</div>


<p><img src="http://image.tmdb.org/t/p/w185/5mP8F0NYSVrwrlWRCSD4uywMNnX.jpg"/></p>

<p><img src="http://image.tmdb.org/t/p/w185/st4P2TtPrAfNwu8HLXoPsPPii42.jpg"/></p>

<p><img src="http://image.tmdb.org/t/p/w185/nZirljb8XYbKTWsRQTplDGhx39Q.jpg"/></p>

<p><img src="http://image.tmdb.org/t/p/w185/6HyFhz2RutIuOAeVOtAXN4cDHbO.jpg"/></p>

<p><img src="http://image.tmdb.org/t/p/w185/zqUFRgbHec3zEMI8jqzau5jYn8Z.jpg"/></p>

<p><img src="http://image.tmdb.org/t/p/w185/sIoe4gjgDk35Ml7857gYSe6P9tG.jpg"/></p>

<p></div>
<div class="jupyter-cell markdown">
</p>

<p>As you can see, maybe we were not using such a good similarity matrix all along. Some of these recommendations are pretty bad - Star Wars is the most similar movie to Toy Story? No other James Bond movie in the top-5 most similar movies to GoldenEye?</p>

<p>One thing that could be the issue is that very popular movies like Star Wars are being favored. We can remove some of this bias by considering a different similarity metric - the pearson correlation. I&rsquo;ll just grab the built-in scikit-learn function for computing this.</p>

<p></div>
<div class="jupyter-cell code">
</p>

<div class="jupyter-input">


<pre><code class="language-python">from sklearn.metrics import pairwise_distances
# Convert from distance to similarity
item_correlation = 1 - pairwise_distances(train.T, metric='correlation')
item_correlation[np.isnan(item_correlation)] = 0.
</code></pre>

</div>


<p></div>
<div class="jupyter-cell markdown">
</p>

<p>Let&rsquo;s look at these movies again.</p>

<p></div>
<div class="jupyter-cell code">
</p>

<div class="jupyter-input">


<pre><code class="language-python">idx = 0 # Toy Story
movies = top_k_movies(item_correlation, idx_to_movie, idx)
posters = tuple(Image(url=get_poster(movie, base_url)) for movie in movies)
display(*posters)
</code></pre>

</div>


<p><img src="http://image.tmdb.org/t/p/w185/uMZqKhT4YA6mqo2yczoznv7IDmv.jpg"/></p>

<p><img src="http://image.tmdb.org/t/p/w185/7f53XAE4nPiGe9XprpGAeWHuKPw.jpg"/></p>

<p><img src="http://image.tmdb.org/t/p/w185/b9QJr2oblOu1grgOMUZF1xkUJdh.jpg"/></p>

<p><img src="http://image.tmdb.org/t/p/w185/b94qXd1FcIwgzv0NYMUe2bjrzJR.jpg"/></p>

<p><img src="http://image.tmdb.org/t/p/w185/vmj2PzTLC6xJvshpq8SlaYE3gbd.jpg"/></p>

<p><img src="http://image.tmdb.org/t/p/w185/tvSlBzAdRE29bZe5yYWrJ2ds137.jpg"/></p>

<p></div>
<div class="jupyter-cell code">
</p>

<div class="jupyter-input">


<pre><code class="language-python">idx = 1 # GoldenEye
movies = top_k_movies(item_correlation, idx_to_movie, idx)
posters = tuple(Image(url=get_poster(movie, base_url)) for movie in movies)
display(*posters)
</code></pre>

</div>


<p><img src="http://image.tmdb.org/t/p/w185/trtANqAEy9dxRCeIe7YEDVeGkLw.jpg"/></p>

<p><img src="http://image.tmdb.org/t/p/w185/duxM3HJ55Fpc4A3krziKn1FR22K.jpg"/></p>

<p><img src="http://image.tmdb.org/t/p/w185/39WsfbB5BshvdbPAYRFXdsjC481.jpg"/></p>

<p><img src="http://image.tmdb.org/t/p/w185/8XefYka77ypAnPJvaVlfUGBBs4a.jpg"/></p>

<p><img src="http://image.tmdb.org/t/p/w185/orGXnBKfT41LxZhitLkXhqUfJJW.jpg"/></p>

<p><img src="http://image.tmdb.org/t/p/w185/k0PN3Ho12cGGIVJW7SCS7apLYaP.jpg"/></p>

<p></div>
<div class="jupyter-cell code">
</p>

<div class="jupyter-input">


<pre><code class="language-python">idx = 20 # Muppet Treasure Island
movies = top_k_movies(item_correlation, idx_to_movie, idx)
posters = tuple(Image(url=get_poster(movie, base_url)) for movie in movies)
display(*posters)
</code></pre>

</div>


<p><img src="http://image.tmdb.org/t/p/w185/5A8gKzOrF9Z7tSUX6xd5dEx4NXf.jpg"/></p>

<p><img src="http://image.tmdb.org/t/p/w185/k3F8N3jeqXOpm1qjY7mL8O6vdx.jpg"/></p>

<p><img src="http://image.tmdb.org/t/p/w185/3pd3sdot0HfQTFzqgTUzaF4kcxP.jpg"/></p>

<p><img src="http://image.tmdb.org/t/p/w185/lP9i3lzCwz8mu8ynuzJJBu4Sa9u.jpg"/></p>

<p><img src="http://image.tmdb.org/t/p/w185/39WsfbB5BshvdbPAYRFXdsjC481.jpg"/></p>

<p><img src="http://image.tmdb.org/t/p/w185/mDhWDGvzy5My1HTBCAFl82gneWN.jpg"/></p>

<p></div>
<div class="jupyter-cell code">
</p>

<div class="jupyter-input">


<pre><code class="language-python">idx = 40 # Billy Madison
movies = top_k_movies(item_correlation, idx_to_movie, idx)
posters = tuple(Image(url=get_poster(movie, base_url)) for movie in movies)
display(*posters)
</code></pre>

</div>


<p><img src="http://image.tmdb.org/t/p/w185/5mP8F0NYSVrwrlWRCSD4uywMNnX.jpg"/></p>

<p><img src="http://image.tmdb.org/t/p/w185/st4P2TtPrAfNwu8HLXoPsPPii42.jpg"/></p>

<p><img src="http://image.tmdb.org/t/p/w185/nZirljb8XYbKTWsRQTplDGhx39Q.jpg"/></p>

<p><img src="http://image.tmdb.org/t/p/w185/sIoe4gjgDk35Ml7857gYSe6P9tG.jpg"/></p>

<p><img src="http://image.tmdb.org/t/p/w185/6HyFhz2RutIuOAeVOtAXN4cDHbO.jpg"/></p>

<p><img src="http://image.tmdb.org/t/p/w185/zqUFRgbHec3zEMI8jqzau5jYn8Z.jpg"/></p>

<p></div>
<div class="jupyter-cell markdown">
</p>

<h4 id="while-the-ordering-changed-some-we-largely-returned-the-same-movies-now-you-can-see-why-recommender-systems-are-such-a-tricky-beast-next-time-we-ll-explore-more-advanced-models-and-see-how-they-affect-the-recommendations">While the ordering changed some, we largely returned the same movies - now you can see why recommender systems are such a tricky beast! Next time we&rsquo;ll explore more advanced models and see how they affect the recommendations.</h4>

<p>For the original IPython Notebook used to generate this post, click <a href="https://github.com/EthanRosenthal/DataPiques_source/blob/master/content/notebooks/2015-11-02-intro-to-collaborative-filtering.ipynb">here</a></p>

</div>


              </article>

              <div class="pagination">
                  
                      <a href="http://ethanrosenthal.com/2015/10/06/interviews-to-job/">&laquo; Yet Another PhD to Data Science Post (Part III)</a>
                  
                  
                      <a href="http://ethanrosenthal.com/2016/01/09/explicit-matrix-factorization-sgd-als/">Explicit Matrix Factorization: ALS, SGD, and All That Jazz &raquo;</a>
                  
              </div>
          </section>
          <br>
          <section class="disqus">
  <div id="disqus_thread"></div>
  <script type="text/javascript">
    (function() {
      
      
      if (window.location.hostname == "localhost")
        return;
      var disqus_shortname = '';
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
  </script>
  <noscript>Please enable JavaScript to view comments powered by <a href="http://disqus.com/?ref_noscript">Disqus</a>.</noscript>
</section>

      </section>

    </main>
    <footer class="row middle-xs center-xs">

  
  
  
    <div class="col-xs-3 col-md-2"><a target="_blank" href="https://github.com/EthanRosenthal">GitHub</a></div>
  

  
    <div class="col-xs-3 col-md-2"><a target="_blank" href="https://linkedin.com/in/ethanrosenthal">LinkedIn</a></div>
  

  
    <div class="col-xs-3 col-md-2"><a target="_blank" href="https://twitter.com/eprosenthal">Twitter</a></div>
  

  
    <div class="col-xs-12">
      
        Copyright &copy; 2018 ethan rosenthal.
      
      
        <a href="https://tomanistor.com" target="_blank">Theme developed by Toma Nistor</a>
      
    </div>
  

</footer>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0-rc.1/dist/katex.min.js" integrity="sha384-483A6DwYfKeDa0Q52fJmxFXkcPCFfnXMoXblOkJ4JcA8zATN6Tm78UNL72AKk+0O" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0-rc.1/dist/contrib/auto-render.min.js" integrity="sha384-yACMu8JWxKzSp/C1YV86pzGiQ/l1YUfE8oPuahJQxzehAjEt2GiQuy/BIvl9KyeF" crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function(){
        	renderMathInElement(document.body, {
                delimiters: [
                    {left: "\\\\begin{equation*}", right: "\\\\end{equation*}", display: true},
                    {left: "$$", right: "$$", display: true},
                    {left: "\\\[", right: "\\\]", display: true},
                    {left: "$", right: "$", display: false},
                    {left: "\\\(", right: "\\\)", display: false}
                ]
            });
        });
    </script>


  <script src="http://ethanrosenthal.com/scripts/main.min.js" type="text/javascript"></script>


  </body>
</html>
