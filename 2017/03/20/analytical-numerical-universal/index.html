<!DOCTYPE html>
<html lang="en-US">
  <head>
  

  <title>
    
    
        From Analytical to Numerical to Universal Solutions | ethan rosenthal
    
  </title>

  <meta name="title" content="From Analytical to Numerical to Universal Solutions | ethan rosenthal">

  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="referrer" content="no-referrer-when-downgrade">
  <meta name="generator" content="">
  <base href="https://ethanrosenthal.com">

  
    <meta name="description" content="I&rsquo;ve been making my way through the recently released Deep Learning textbook (which is absolutely excellent), and I came upon the section on Universal Approximation Properties. The Universal Approximation Theorem (UAT) essentially proves that neural networks are capable of approximating any continuous function (subject to some constraints and with upper bounds on compute).
Meanwhile, I have been thinking about the modern successes of deep learning and how many computer vision researchers resisted the movement away from hand-defined features towards deep, uninterpretable neural networks.">
  

  
    <meta name="author" content="Ethan Rosenthal">
  

  
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:site" content="@eprosenthal">
    <meta name="twitter:creator" content="@eprosenthal">
  

  <meta property="og:title" content="From Analytical to Numerical to Universal Solutions | ethan rosenthal">
  <meta property="og:type" content="website">
  <meta property="og:url" content="https://ethanrosenthal.com">

  
    <meta property="og:image" content="https://ethanrosenthal.com/images/profile_pic2.jpg">
  

  
    <meta name="og:description" content="I&rsquo;ve been making my way through the recently released Deep Learning textbook (which is absolutely excellent), and I came upon the section on Universal Approximation Properties. The Universal Approximation Theorem (UAT) essentially proves that neural networks are capable of approximating any continuous function (subject to some constraints and with upper bounds on compute).
Meanwhile, I have been thinking about the modern successes of deep learning and how many computer vision researchers resisted the movement away from hand-defined features towards deep, uninterpretable neural networks.">
  

  
    <link rel="icon" type="image/png" sizes="16x16" href="favicon.ico">
    <meta name="theme-color" content="#FFF">
  

  <link rel="canonical" href="https://ethanrosenthal.com/2017/03/20/analytical-numerical-universal/">

  

  <link rel="stylesheet" href="https://ethanrosenthal.com/styles/main.css" type="text/css">

  

  
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
  

  

  

  
</head>

  <body>
    



<nav class="row middle-xs center-xs">
  <div class="col-xs-6 col-sm-1 logo">
    <a href="https://ethanrosenthal.com#"><img src="https://ethanrosenthal.com/images/profile_pic2.jpg" alt="ethan rosenthal"></a>
  </div>
    
      <div class="col-xs-3 col-sm-2">
        <h3><a href="https://ethanrosenthal.com/#blog">Blog</a></h3>
      </div>
    
      <div class="col-xs-3 col-sm-2">
        <h3><a href="https://physics.ethanrosenthal.com">Physics</a></h3>
      </div>
    
      <div class="col-xs-3 col-sm-2">
        <h3><a href="https://ethanrosenthal.com/#about">About</a></h3>
      </div>
    
  <div class="col-xs-6 col-sm-1 nav-toggle">
      <a href="" class="nav-icon" onclick="return false"><img src="https://ethanrosenthal.com/images/icon-menu.png" alt="Open Menu"><img src="https://ethanrosenthal.com/images/icon-x.png" alt="Close Menu" style="display: none;"></a>
  </div>
</nav>

<section class="nav-full row middle-xs center-xs">
  <div class="col-xs-12">
    <div class="row middle-xs center-xs">
      
        <div class="col-xs-12"><h1><a href="https://ethanrosenthal.com/#blog">Blog</a></h1></div>
      
        <div class="col-xs-12"><h1><a href="https://physics.ethanrosenthal.com">Physics</a></h1></div>
      
        <div class="col-xs-12"><h1><a href="https://ethanrosenthal.com/#about">About</a></h1></div>
      
    </div>
  </div>
</section>

    <main>

      <section class="container">
          <section class="content">
              <h1> From Analytical to Numerical to Universal Solutions </h1>

              <div class="sub-header">
                  March 20, 2017 · 3 minute read
              </div>

              <article class="entry-content">
                  <p>I&rsquo;ve been making my way through the recently released <a href="http://www.deeplearningbook.org/">Deep Learning</a> textbook (which is absolutely excellent), and I came upon the section on Universal Approximation Properties. The <a href="https://en.wikipedia.org/wiki/Universal_approximation_theorem">Universal Approximation Theorem</a> (UAT) essentially proves that neural networks are capable of approximating any continuous function (subject to some constraints and with upper bounds on compute).</p>

<p>Meanwhile, I have been thinking about the modern successes of deep learning and how many computer vision researchers resisted the movement away from hand-defined features towards deep, uninterpretable neural networks. By no means is computer vision the first field to experience such existential angst. Coming from a physics background, I recall many areas that slowly moved away from expert knowledge towards less-understood, numerical methods. I wonder how physicists dealt with such Sartrean dilemmas?</p>

<p>I think deep learning might be different, though.</p>

<p>To illustrate my point, it is helpful to think of the various methods for solving scientific, mathematical problems as existing on a spectrum. On one side is a closed-form, analytical solution. We express a scientific model with mathematical equations and then solve that problem analytically. Should these solutions resemble reality, then we have simultaneously solved the problem and helped to confirm our scientific understanding. Pretty much any introductory physics problem, like the kinematic equations, falls on this side of the spectrum.</p>

<p>On the other side of the spectrum, we have a complete black box solution. We put our inputs into the box, and we get some outputs back out which have fit our function or solved our problem. We know nothing about what is going on inside.</p>

<p>What falls towards the middle of the spectrum? There are many areas of science and applied math which live here, many of which consist of models expressed as differential equations with no analytical solution. With the lack of analytical solution, one must resort to numerical methods for solving these problems. Examples here abound: <a href="https://en.wikipedia.org/wiki/Density_functional_theory">Density Functional Theory</a>, <a href="https://en.wikipedia.org/wiki/Finite_element_method">Finite Element Analysis</a>, <a href="https://en.wikipedia.org/wiki/Reflection_seismology">Reflection Seismology</a>, etc&hellip;</p>

<p>What is intersting about deep learning is that it is now being <a href="http://www.nature.com/nphys/journal/vaop/ncurrent/full/nphys4053.html">used</a> to tackle these middle-ground, numerical problems. Hell, there is a <a href="https://arxiv.org/abs/1702.01361">paper</a> on the arxiv using deep learning to solve the Schrödinger equation!</p>

<p>At least with the original numerical methods for solving differential equations, one had to do a decent job of modeling (and presumably understanding) the system before using a computer to solve the problem. What remains to be seen is whether or not this will still be true with deep learning. If not, then it feels like we are entering true black box territory. What I wonder is if the analytical side of the spectrum will even be necessary? Is intuition and domain knowledge necessary for innovation and pushing the boundaries of problem solving? Or, can this Universal Approximator break away from the pack and go on crushing records indefinitely?</p>

<p>Ideally, I would like to believe that we need both. Insights from domain knowledge help drive new approaches (e.g. neural networks were originally <em>inspired</em> by the brain), and breakthrough, black box results help us to understand the domain (not as many examples of these, yet&hellip;).</p>

<p>In the meantime, while everybody grapples with this new technology and publishes marginal increases in MNIST accuracy, it seems like the old, differential equation-heavy fields are up for grabs for anybody who can suitably aim the deep learning hammer.</p>

              </article>

              <div class="pagination">
                  
                      <a href="https://ethanrosenthal.com/2017/02/05/rec-a-sketch/">&laquo; Rec-a-Sketch: a Flask App for Interactive Sketchfab Recommendations</a>
                  
                  
                      <a href="https://ethanrosenthal.com/2017/06/20/matrix-factorization-in-pytorch/">Matrix Factorization in PyTorch &raquo;</a>
                  
              </div>
          </section>
          <br>
          <section class="disqus">
  <div id="disqus_thread"></div>
  <script type="text/javascript">
    (function() {
      
      
      if (window.location.hostname == "localhost")
        return;
      var disqus_shortname = '';
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
  </script>
  <noscript>Please enable JavaScript to view comments powered by <a href="http://disqus.com/?ref_noscript">Disqus</a>.</noscript>
</section>

      </section>

    </main>
    <footer class="row middle-xs center-xs">

  
  
  
    <div class="col-xs-3 col-md-2"><a target="_blank" href="https://github.com/EthanRosenthal">GitHub</a></div>
  

  
    <div class="col-xs-3 col-md-2"><a target="_blank" href="https://linkedin.com/in/ethanrosenthal">LinkedIn</a></div>
  

  
    <div class="col-xs-3 col-md-2"><a target="_blank" href="https://twitter.com/eprosenthal">Twitter</a></div>
  

  
    <div class="col-xs-12">
      
        Copyright &copy; 2018 ethan rosenthal.
      
      
        <a href="https://tomanistor.com" target="_blank">Theme developed by Toma Nistor</a>
      
    </div>
  

</footer>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>


  <script src="https://ethanrosenthal.com/scripts/main.min.js" type="text/javascript"></script>


  </body>
</html>
